[Назад](./README.md)

1. [Гарантии согласованности](#гарантии-согласованности)  
2. [Линеаризуемость](#линеаризуемость)  
3. [Гарантии упорядоченности](#гарантии-упорядоченности)  
4. [Рассылка общей последовательности](#рассылка-общей-последовательности)  
5. [Двухфазная фиксация](#двухфазная-фиксация)  
6. [Консенсус](#консенсус)  
7. [Пару слов про Zookeeper](#пару-слов-про-zookeeper)  
8. [Консенсус и Postgres](#консенсус-и-postgres)  
9. [Резюме](#резюме)

# Согласованность и консенсус
Согласованность в системе БД - обязательная для решения задача в системах распределенного хранения данных. Существует 
несколько способов реализации согласованности, ниже мы рассмотрим несколько из них. 

Однако согласованность не решает всех проблем с распределенным хранением и обработкой данных. Зачастую для нескольких
узлов необходимо составить единое решение о какой-то проблеме конкурентной работы. Решением этой проблемы является 
консенсунс. Обеспечение консенсуса - удивительно сложная задача.

## Гарантии согласованности
Большинство реплицированных БД гарантируют конечную согласованность (eventually consistency), однако это очень слабая 
гарантия. Она ничего не говорит о том, когда данные будут консистентны и до этого момента операции чтения возвращать
что угодно или вообще ничего. При работе с такой БД надо постоянно помнить об этих ограничениях. Такое поведение очень
сильно отличается от классического однопоточного выполнения программ, когда свежезаписанную переменную можно прочитать
и получить новое значение. Отсюда часто возникают ошибки, которые довольно сложно отловить на этапе тестирования.

Существуют более сильные модели согласованности. У каждой из них есть своя цена: системы с более сильной согласованностью
имеют худшую производительность или могут быть менее отказоустойчивыми.

## Линеаризуемость
Линеаризуемость - свойство, которое гарантирует, что любая операция чтения или записи в базу данных будет выполнена
атомарно и в определенном порядке. Например, если два пользователя пытаются изменить одну и ту же запись в базе данных,
то линеаризуемость гарантирует, что только одно из этих изменений будет выполнено первым, а второе будет выполнено после
того, как первое изменение будет завершено. Это предотвращает возможные конфликты и ошибки в данных. Под капотом
линеаризуемость работает при помощи блокировок и транзакций.

> :exclamation: **Линеаризуемость - самая сильная гарантия согласованности.**

В линеаризуемой системе предполагается, что есть некий момент времени (во время записи), при котором значение меняется 
со старого на новое. После этого момента времени _все_ последующие операции чтения обязаны возвращать новое значение.

Список областей, в которых может понадобиться линеаризуемость:
1) **Блокировка и выбор ведущего узла**: у всех узлов в системе должно быть единое понимание, кто является 
ведущим узлом. На деле чаще всего это решается при помощи инструментов по типу Zookeeper, который используется 
для решения консенсусный алгоритм.
2) **Ограничения и гарантия уникальности**: чтобы поддерживать уникальность некоторого поля, нужна линеаризуемость, ведь
в случае конкурентной записи необходимо однозначно определить, кто запишет значение первым, а кому не дать записать
повторяющееся значение.
3) **Межканальные синхронизационные зависимости**: в случае если один сервис записывает данные в БД, а другой из нее 
читает (тут же или через некоторое время), эта БД должна быть линеаризуема, чтоб второй сервис смог получить
свежезаписанные данные.

Реализация линеаризуемых систем:
1) **Репликация с одним ведущим узлом**: потенциально линеаризуемо. Если чтение происходит из мастера или 
синхронизированных реплик, то такая система линеаризуема. Однако на практике этому может помешать архитектура БД или 
ошибки возникающие из-за конкурентных операций. Архитектурно мешают следующие нюансы: чтение из всех реплик, 
неправильный выбор нового мастера, изоляция снимков состояния, етс.
2) **Консенсусные алгоритмы**: БД построенные на основе консенсусных алгоритмов (Zookeeper) тоже могут быть 
линеаризированы. Все благодаря алгоритмам, которые лежат в их основе, их мы рассмотрим дальше.
3) **Репликация с несколькими ведущими узлами**: не являются линеаризуемыми из-за их конкурентной природы записи.
4) **Репликация без ведущего узла**: иногда можно получить подобие линеаризуемости, если правильно настроить 
кворум (w + r > n).

Важно знать про **цену линеаризуемости**, коротко ее можно выделить так:
1) Если приложение _требует_ линеаризуемость, то в случае проблем в сети часть реплик могут стать недоступными (потому что
не могут связаться с другими репликами для синхронизации). Таким образом, мы _теряем доступность_.
2) Если приложение _не требует_ линеаризуемость, то в случае проблем в сети реплики продолжат работу (потому что
им больше не нужно синхронизироваться). Таким образом мы _получаем высокую доступность_, хоть и теряем линеаризуемость.

Если совсем коротко, то приложения не требующие линеаризуемости, более устойчивы к неполадкам в сети. Это представление
более широко известно как **CAP теорема**.

Аббревиатура CAP расшифровывается как Consistency, Availability, Partition Tolerance (Согласованность, Доступность,
Устойчивость к нарушению связности). При нарушении связности можно выбрать только что-то одно: согласованность данных
или доступность БД в целом. Таким образом, любая система БД может быть AP или CP.

Зачастую в современных системах отказываются от линеаризуемости, только не в угоду доступности, а в угоду 
_быстродействию_. Именно поэтому сейчас активно приходят новые теоремы на смену CAP. В реальных сетях с задержками
линеаризуемость работает **медленно**. Быстрых алгоритмов для линеаризуемости еще не придумали, однако можно более слабые
модели согласованности данных и таким образом ускорить систему. 

## Гарантии упорядоченности
Более слабая версия согласованности - упорядоченность (причинность, ordering). Даная гарантия упорядочивает зависимые
операции, но допускает параллельную обработку независимых операций. Можно провести параллель с git, где коммиты в одной
ветки строго упорядоченны, но коммиты из разных веток могут быть параллельны. Можно сказать, что линеаризуемость - 
абсолютная упорядоченность, поскольку любые два действия будут упорядочены относительно друг друга, мы уже рассматривали,
к каким накладным расходам это может привести. Зачастую системам хватает упорядоченности для правильной и быстрой работы.

Для сохранения причинно-следственных связей используются _порядковые номера_ или _временные метки_. Такая метка 
ставится не по физическим часам, а по логическим часам - алгоритм генерации последовательности чисел для 
идентификаций операций. Такая метка является компактной и может обеспечивать полную упорядоченность, поскольку любые
две операции имеют свой номер и по нему можно понять порядок операций. Конкурентные операции могут быть упорядочены
произвольно.

В базе данных с одним ведущим узлом именно он монотонно генерирует эту последовательность номеров. Таким образом, если
реплика выполняет операции в порядке согласно номерам, то его состояние всегда является причинно-упорядоченным. 

Если единого ведущего узла нет, то можно использовать метод временных меток Лампорта: каждый узел имеет ID и хранит 
увеличивающийся счетчик операций. Данный счетчик передается во всех запросов и ответах. Если узел или клиент встречают
значение счетчика, большего своего, они немедленно увеличивают свой счетчик до этого значения. Если у двух узлов
одинаковое значение счетчика, то при конкурентном запросе сначала обработается узел с меньшим ID. 

![img.png](../../../../img/highload/lamport.png)

Несмотря на то, что метки Лампорта определяет полную последовательность счетчика согласованную с причинностью, их 
недостаточно для решения многих проблем. Например, если два пользователя хотят создать аккаунт с одинаковым именем 
пользователя, это должно получится только у одного из них. На первый взгляд кажется, что меток Лампорта хватит: 
завершается только та операция, которая имеет меньшую метку, а другая отклоняется. Однако такой метод работает только 
для сравнения постфактум, сначала надо собрать все операции создания аккаунтов, а потом можно сравнивать их временные 
метки. Этого недостаточно, если узлу необходимо принять решение моментально при получении запроса. 

Проблема в следующем: полное упорядочение операций происходит после того, как были собраны все операции. Если при 
записи уникального значения ни один другой узел не может заявить права на то же значение (но с более ранней временной 
меткой), то такую операцию можно считать успешной. Эта идея - знать когда общая последовательность построена - 
реализована в методе _рассылки общей последовательности_. 

## Рассылка общей последовательности
Рассылка общей последовательности - протокол обмена сообщениями между узлами, для него есть 2 требования:
1) _Надежная доставка_ - ни одно сообщение не должно быть потеряно. Если сообщение доставлено одному узлу, оно будет
доставлено всем узлам.
2) _Полностью упорядоченная доставка_ - все сообщения доставляются во все узлы в одном и том же порядке.

Для рассылки общей последовательности выбирается 1 узел, который будет получать запросы и рассылать их всем остальным 
узлам.

Рассылка общей последовательности решает многие вопросы (репликация, консенсунс, етс), сервисы консенсуса (Zookeeper)
реализовывают рассылку общей последовательности. Важным ее аспектом является то, что последовательность фиксируется в 
момент доставки и нельзя вставить данные задним числом. 

Линеаризуемый реестр с атомарным приращением счетчика и рассылка общей последовательности эквивалентны консенсусу. Иными
словами, если одна из указаных проблем решена, то она превращается в решение для других. 

## Двухфазная фиксация
Распределенные транзакции бывают двух видов:
1) Внутренние - распределенная транзакция между несколькими узлами одной БД. Поддерживают не все БД.
2) Гетерогенные - распределенная транзакция между разными технологиями и производителями хранения данных.

Под распределенными транзакциями мы будем понимать гетерогенные распределенные транзакции.

**2PC** (two-phase commit) - популярный алгоритм двухфазной фиксации распределенных транзакций. На самом деле, 2PC является
своего рода консенсусным алгоритмом (хоть и не очень хорошим). Так же, Zookeeper и etcd построены на базе консенсусных
алгоритмов. 

В одноузловых транзакциях за атомарную фиксацию отвечает одно устройство на одном узле. Если узлов несколько, то 
недостаточно просто отправит запрос на фиксацию всем узлам. Может произойти так, что на одних узлах транзакция применится,
а на других откатится и как следствие, нарушится гарантия атомарности. Такие узлы станут несовместимы между собой, не 
получится отменить уже зафиксированную транзакцию на части узлов.

2PC гарантирует, что все узлы либо зафиксируют транзакцию, либо прервут ее. Вместо одного запроса на фиксацию, в 2PC 
процесс разбит на две фазы (подготовка, фиксация). 2PC для этого использует новый компонент - _координатор_. Двухфазные 
фиксации реализованы поверх **стандарта XA** (eXtended Architecture). По сути это интерфейс, который реализуют
координаторы транзакций в 2PC решениях. Данный стандарт поддерживают все популярные СУБД и языки программирования.  
![img.png](../../../../img/highload/2pc.png)

Когда приложение готово к фиксации, координатор начинает этап 1: он отправляет запрос на подготовку каждому из узлов,
спрашивая их, могут ли они выполнить фиксацию. Каждый участник проверяет, что точно может завершить транзакцию при 
любых обстоятельствах (даже в случае поломки диска). Отвечая "Да" участник обещает зафиксировать транзакцию без 
ошибок в случае надобности. _После этого у участника **нет права** отменить транзакцию_. После получения ответов всех
участников, координатор записывает решение себе на диск, чтобы в случае сбоя понять, какое решение было принято (это 
называется точка фиксации). Если все узлы могут, то на этапе 2 координатор отправляет запрос фиксации. Если хотя бы 
один участник не может, то на этапе 2 всем узлам отправляет запрос прерывания. Если запрос до участника не дошел, то 
координатор будет пытаться отправить его снова и снова. Это точка невозврата: в случае принятия решения оно должно быть
исполнено. 

Две точки невозврата: ответ участника "Да" и принятие решения координатором обеспечивают атомарность 2PC.

Если с координатором случится сбой перед отправкой запросов на подготовку, то участник может безопасно прервать
транзакцию. Но как только участник получил запрос на подготовку и ответил "да", он больше не может самостоятельно 
отменить транзакцию. В таком случае участнику остается только ждать, а такая транзакция называется сомнительной или 
неопределенной. Единственный способ завершения процедуры - ожидание восстановления координатора. Вот почему он 
записывает свои решения на диск, прежде чем отправлять участникам запрос на фиксацию или откат.

Транзакция, застрявшая в состоянии неопределенности, представляет собой большую проблему. Такая транзакция блокируют 
данные для чтения и изменения другим транзакциям, поэтому важно уделять большое внимание отказоустойчивости 
координатора. Если координатор по какой-то причине не может решить, что делать с зависшей транзакцией, то в дело 
вступает администратор, который в ручном режиме будет разбираться с проблемой.  

![img.png](../../../../img/highload/2pc_crash.png)

Из-за этой проблемы 2PC называют блокирующий протоколом атомарной фиксации. В качестве альтернативы предложен алгоритм
трехфазной фиксации (3PC), однако он требует наличие сети с ограниченной задержкой и узлами с ограниченным временем 
отклика. В большинстве систем с неограниченными сетевыми задержками и паузами процессов он не гарантирует атомарность. 
В общем случае для неблокирующего протокола нужна идеальная сеть (или идеальный детектор отказов), что на практике 
нереализуемо, поэтому все продолжают использовать 2PC.

На практике, распределенные транзакции (особенно реализованные с помощью двухфазной фиксации) имеют смешанную репутацию.
Их критикуют за то, что они вызывают проблемы и убивают производительность. Распределенные транзакции MySQL в 10 раз
медленнее одноузловых. Многие пытаются обойтись без распределенных транзакций.

## Консенсус
Цель **консенсуса** - заставить несколько узлов согласовать некие объекты (принять общее решение). Обычно узлы предлагают
свои решения, а консенсусный алгоритм выбирает одно из них. Ситуации, в которых важно, чтобы узлы были согласованны:
1) Выбор ведущего узла
2) Распределенные транзакции (единое решение о коммите или роллбеке между узлами)

Консенсусный алгоритм удовлетворяет следующим **требованиям**:
- Единое решение: никакие два узла не могут получить разные решения
- Целостность: никакой узел не получит два решения
- Действительность: если узел выбирает решение V, то V было предложено одним из узлов
- Завершенность: каждый узел, который не вышел из строя, в конечном итоге выбирает то или иное значение

Первые 3 требования самые легкие и их легко выполнить, это мы и наблюдали в двухфазной фиксации. Достаточно просто 
ввести в схему узел-диктатор, который будет принимать решения. Однако с выходом из строя этого узла у системы
начинаются проблемы. Такой алгоритм не выполняет требование завершенности, которое говорит, что консенсусный алгоритм не 
может просто остановиться и ничего не делать - он должен всегда двигаться вперед. Что если узел вышел из строя навсегда?
В таком случае система навсегда останется в нерабочем состоянии. 

Наиболее известными отказоустойчивыми алгоритмами являются VSR, Paxos, Raft и Zab. Большинство алгоритмов построены на 
основе _рассылки общей последовательности_, выбирается узел, который будет рассылать запросы остальным узлам. Для такой 
рассылки требуется, чтобы сообщения были доставлены всем узлам ровно один раз в одинаковом порядке. Таким образом, 
рассылка общей последовательности эквивалентна нескольким циклам консенсуса:
- Единое решение: все узлы получают одни и те же сообщения в одинаковом порядке
- Целостность: сообщения не дублируются
- Действительность: сообщения не повреждаются и не появляются из ниоткуда
- Завершенность: сообщения не теряются

По сути консенсусный алгоритм реализованный при помощи рассылки общей последовательности напоминает репликацию с одним
ведущим узлом, а для этой репликации необходим ведущий узел, который нужно выбрать с помощью консенсуса... Походу для 
консенсуса нам необходим консенсус? Не совсем. Консенсусные алгоритмы предлагают более слабую гарантию, они определяют
лидера на период. Каждый раз, когда предыдущий лидер умер (перестал отправлять heartbeat остальным узлам), узлы начинают
новое голосование на ведущий узел. Каждым таким выборам присваивается порядковый номер и очередной ведущий узел
является ведущим только в этом периоде. Если какой-либо узел узнает (из запросов и ответов), что у него устарел номер 
периода, он обновляет его. Бывший ведущий узел в таком случае понимает, что он больше не ведущий.

Решение, которое принимает ведущий узел, не является безоговорочным. Ведущий узел не в курсе, возможно его признали 
умершим и он более не является ведущим узлом. По этой причине ведущий узел лишь предлагает какое-то решение и кворум 
(большинство узлов) должны принять это решение. Если был выбран новый ведущий узел, то большинство узлов в курсе про
это событие, предложение старого ведущего узла не будет принято. Если новый ведущий узел предлагает изменение, которое
ломает согласованность данных, то это кворум не примет это изменение. 

Ключевыми киллер фичами консенсусного алгоритма являются:
1) Ведущий узел на период избирается голосованием
2) Для согласования изменений необходимо большинство голосов (не обязательно все)
3) Предусмотрен процесс восстановления после падения ведущего узла

Несмотря на все плюсы консенсусных алгоритмов, их не применяют повсеместно, потому что за их использование приходится 
платить:
1) Из-за кворумов необходимо большое число узлов, чтобы пережить сбои (три узла, чтобы пережить сбой одного). Иначе 
работа системы будет заблокирована, кворум не будет собран.
2) Большинство консенсусных алгоритмов предполагают, что в голосовании участвует фиксированный набор узлов. Динамическое 
добавление возможно, но оно менее понятно, чем статическое использование.
3) В случае сетевых проблем будут частые выборы нового ведущего узла, что приводит к большим временным издержкам.

## Пару слов про Zookeeper
Функциональные способности Zookeeper:
1) Линеаризуемые атомарные операции
2) Упорядоченность последовательности операций (у каждой операции свой zxid)
3) Прослушивание heartbeat всех участников системы
4) Рассылка обновлений кластера заинтересованным участникам

Из этих способностей только линеаризуемые атомарные операции требует консенсуса. Однако сочетание описанных
функций делают Zookeeper полезным ждя координации в распределенных средах. Они понадобятся в решении следующих задач:
1) Выбор нового ведущего узла. Распределение партиций между узлами. 
2) Обнаружение сервиса (выяснение, по какому адресу надо обратиться, чтобы обратиться к сервису).
3) Решение, какие узлы считать умершими, а какие нет.

# Консенсус и Postgres
Postgres из коробки поддерживает только репликации с одним ведущим узлом. В случае если ведущий узел выходит из строя,
необходимо в ручном режиме запустить процесса failover, когда один из ведомых синхронных узлов станет новым мастером.
Таким образом из коробки Postgres не использует никакой консенсусный алгоритм.

Однако для постгреса есть инструменты автоматизации данного процесса от сторонних разработчиков, например Patroni и
repmgr. Данные инструменты автоматически запускают процесс выбора нового мастера и используют etcd или Zookeeper под
капотом.

# Резюме
В целом вся глава была про согласованность и консенсус. Мы рассмотрели линеаризуемость - самую сильную модель 
согласованности данных. Цель этой модели: реплицированные данные выглядят так, как будто существует только одна копия 
и все операции воздействуют на нее атомарно. Несмотря на привлекательность линеаризуемости, ее большой недостаток в 
замедлении работы системы. Существует модель согласованности послабее - причинность. Она упорядочивает только зависимые 
друг от друга операции. Модель сильно напоминает git ветки. Причинная согласованность меньше замедляет работу системы. 

Однако не все системы могут быть построены с помощью этой технологии. Допустим нам надо регистрировать аккаунты и 
следить, чтобы имена аккаунтов были уникальны. В таком случае узлы должны между собой принять решение. Эта идея привела
нас к консенсусу. Консенсус позволяет принять решение, с которым будут согласны все узлы и будет неотменяемым.

Выяснилось, что есть широкий список проблем, который сводится к консенсусу:
1) **Линейные реестры сравнения с присвоением**. Реестр должен атомарно принять решение, присваивать ли значение в 
зависимости от того, соответствует текущее значение параметру, указанному в операции.
2) **Атомарная транзакция**. Следует завершать или отменить распределенную транзакцию.
3) **Рассылка общей последовательности**. Принятие решения о последовательности доставки сообщений в системе.
4) **Блокировка или аренда**. Когда несколько клиентов хотят заблокировать ресурс.
5) **Сервис членства и координации**. Принятие решения о том, какие узлы активны, а какие нет.
6) **Ограничение уникальности**. Когда несколько транзакций конкурентно пытаются создать запись с повторяющимся 
значением, которое должно быть уникально. 

Всего описанного очень просто добиться, если узел, принимающий решения, один. Такая ситуация в репликации с одним
ведущим узлом. Поэтому такая реплика могут предоставлять линеаризуемые операции, ограничения уникальности, 
упорядоченный журнал репликации и тд. Однако если этот узел выходит из строя, то такая система становится 
неработоспособной. Существует 3 способа выхода из данной ситуации:
1) Дождаться пока ведущий узел восстановиться и согласиться, что все это время система будет заблокирована. 
2) Решить проблему вручную - новый мастер узел выбирает человек.
3) Использовать алгоритм автоматического выбора нового ведущего узла. Такой вариант требует консенсусного алгоритма. 

Несмотря на то, что база данных с одним ведущим узлом способа обеспечить линеаризуемость без консенсусного алгоритма, 
она по-прежнему требует консенсуса, только для выбора нового ведущего узла. Получается консенсус все равно необходим,
только реже. Хорошая новость в том, что существуют надежные консенсусные алгоритмы и системы поверх них (например 
Zookeeper). 

Такие инструменты как Zookeeper играют важную роль в обеспечении аутсорсинга консенсуса, обнаружении сбоев и 
обслуживании участников системы. Данная система непроста, но это лучше, чем городить свои велосипеды. Если когда-нибудь
придется решать одну из задач консенсуса, лучше всего использовать Zookeeper и подобные инструменты. 

Однако не каждая система требует консенсуса: системы без ведущего узла или с несколькими такими узлами обычно не 
используют глобальный консенсус. Конфликты в таких системах являются следствием согласия между разными ведущими узлами,
иначе говоря, это их особенность. Такие конфликты решаются другими способами (бизнес процессом и кодом).